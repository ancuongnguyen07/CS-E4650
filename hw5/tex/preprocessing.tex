\section{Preprocessing}

The preprocessing of the language data started with a reading the CSV file and placing it in a Pandas dataframe with columns ID, Title, Abstract, and Text. The column text was a concatenation of the Title and the Abstract. The next steps will be applied on the combined column Text, since that takes into account both the Title and the Abstract, and both of those give information of how to cluster the documents.

The text is then tokenized using the NLTK library, which stands for the Natural Language Toolkit. After tokenization all text is turned into lowercase and numbers are removed. Next the punctuation is removed through comparing the text with the punctuation variable from the python base library string. For removing stopwords, we use the basic stopwords list from NLTK with option `english' and also update the list with the ellipsis symbol. This step comprises all filtering that is done in preprocessing. The data is later filtered more with an expanded stopwords list based on the observations of the cluster digest terms.


The reason why the lowercasing and other preprocessing operations are done is that they, e.g., help matching words that only differ in their case. This makes it easier to find similarities between the texts.


After the the cleaning up of the tokens, the operation of stemming is applied. Stemming will shorten the words to their base or root form \autocite[pages 431--434]{aggarwal2015data}. Example of stemming for words ``playing'', ``played'', ``player'', which all get stemmed to their base form of ``play''. The stemmer used is the SnowballStemmer, which is an improved version of the Porter stemmer which itself is already one of the most widely used stemmers for English \autocite{porter1980algorithm}. After stemming all tokens get lemmatized which means that through a lookup table and morphological analysis, the words get turned into their base form. The difference with stemming is that lemmatization makes sure that the ouputted base form words are valid words.




\subsection{Order of preprocessing steps}

The first steps are logically placed for stemming or lemmatization to reduce the special symbols and stopwords. The internal order of lowercasing and number and punctuation removal is irrelevant since they are independent operations that do not affect each other. The stopword deletion should be done after lowercasing, since the stopword list is lowercase. Stopwords should be applied before stemming or lemmatization to ensure that proper stopwords are not modified before they are given to the stopword remover. However, there is a fair question of lemmatization and stemming order. It can be argued that stemming first will reduce the available lemmatization and thus reducing the accuracy of the words. However, by stemming first we might enable better computational performance since lemmatization requires more operations per a single word. However, it is not certain if this is the optimal way to combine lemmatization and stemming. For future research, it would be interesting to see the difference of lemmatization only vs stemming + lemmatization for large corpora of data. Additional improvement ideas for the preprocessing could be the replacement of synonyms with a common synonym, thus reducing the distance between texts from authors that use words that have the same meaning but a different string representation.


\subsection{Vectorization}

The rerpresentation of the tokens is then turned into a matrix form, through the TfidVectorizer from the \emph{sklearn.feature\_extraction.text} library. This function creates a matrix that has the documents in the rows and the columns include every unigram and a bigram which pass the required frequency we have set. The requirement for the n-grams to appear on columns is that they need to appear at most in 80\% of the documents and at least in 5 documents. This requirement will prune out words that don't have a high significance in distinguishing different documents from each other. The columns are then scaled to be unit vectors according to their L2 norm. This scaling ensures that the difference in absolute values of the columns don't impact the clustering.



The formula for TF-IDF used in the vectorizer:
\[
    \text{TF-IDF}(t, d, D) = \text{TF}(t, d) \times \text{IDF}(t, D)
\]

\[
    \text{TF}(t, d) = \frac{\text{Number of occurrences of term } t \text{ in document } d}{\text{Total number of terms in document } d}
\]

\[
    \text{IDF}(t, D) = \log\left(1 + \frac{N}{1 + n_t}\right)
\]

where:
\begin{itemize}
    \item \(t\): Term (word or n-gram).
    \item \(d\): A single document.
    \item \(D\): The collection of documents (corpus).
    \item \(N\): Total number of documents in the corpus.
    \item \(n_t\): Number of documents containing the term \(t\).
\end{itemize}
